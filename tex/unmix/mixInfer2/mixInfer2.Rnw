\documentclass{article}

\usepackage{hyperref}

\newcommand{\funname}[1]{\texttt{#1}}

\title{More on unmixing}
\author{Josh Burdick}

\begin{document}

\maketitle

<<intro,echo=FALSE>>=
library("MASS")
wd = getwd()
setwd("../../../..")
source("git/unmix/ept/practice/simplex_corner.r")
source("git/unmix/ept/gamma.r")
setwd(wd)
setwd("../../../..")
setwd("git/unmix/unmix_comp/src/")
# print(getwd())
source("sampling/cdaCpp.r")
setwd(wd)

@

\section{Another way of estimating marginals}

We have been treating unmixing as a constrained linear system

\[
Ax = b, x \ge 0
\]

This defines a space of solutions; we then chose $x$ uniformly
at random from that space, and used that distribution as the range
of possible levels of expression. Previously, we used sampling
and EP to estimate these. EP was much faster, but had convergence
issues. I tried to alleviate the convergence problems by approximating the
marginals using gamma distributions, rather than a multivariate
normal distribution. This is partly motivated by the fact that the
actual marginals (from sampling) look more like gamma distributions
than normal distributions.

The solutions to the above form a convex linear space. As such,
if we can find the corners of that region, the boundaries between
the corners are straight lines, and we can linearly transform the
region to the standard unit simplex

\[
\sum y = 1, y \ge 0
\]

Note that this has the same number of ``corners'' as the original
space, and this will be less than $n$ (the ``number of cells'', or
length of $x$.) The moments of this are easy to find: if $m$ is
the number of corners,
then each marginal distribution
has a $\mathrm{Beta}(m-1, 1)$ distribution.

For now, in order to find the corners, we just maximize each 
coordinate separately. This gives $n$ points, but only $m$ of them
are unique. We just pick distinct points based on distance.

Each maximization is a linear

\subsection{Accuracy of that approximation}

To validate this approach, we compare its marginals to those from
sampling (at least on some toy examples.)

First, we design a very toy problem:

<<toy1,echo=TRUE>>=
A = matrix(sample(0:5, 32, replace=TRUE), nrow=4)
A
x = sample(c(0,1,2,4,8), 8, replace=TRUE)
x
b = as.vector(A %*% x)
b
@

We compute marginals using this method.

<<infer1,echo=TRUE>>=
x.infer = approx.region.simplex.transform(A,b)
round(x.infer,2)
round(as.vector(A %*% x.infer["m",]), 2)
@

So, at least the mean matches the constraints.

<<sample1,echo=FALSE>>=
x0 = x.infer["m",]
x.sample = sample.cda(A, b, x0, 1e5, 100)
@

We then compare with sampling.
By eye, the marginals estimated this way (in blue) fit the actual
densities from sampling (in grey) fairly well.

<<argCompare1,dev='pdf',fig.width=8,fig.height=4,echo=FALSE>>=
par(mfrow=c(2,4))
xlim=range(x.sample)
x = xlim[2] * (c(0:100)/100)
g = gamma.mv2s(x.infer)
for(j in 1:8) {
  hist(x.sample[,j], xlim=xlim, breaks=200, col="#00000010", border="#00000020")
  par(new=TRUE)
  plot(x, dgamma(x, shape=g[1,j], rate=g[2,j]), type="l",
    col="#0000ff", lwd=2,
    xlab="", ylab="", main="", xlim=xlim, xaxt="n", yaxt="n")
}
@

\section{Modelling uncertainty in the sort matrix}

Solving the original problem, in a deterministic (and probably faster) way is nice.
However, the $x \ge 0$ constraint is problematic: while it gives a way of estimating
uncertainty, it also means that if the sort matrix disagrees too much with the expression
data, there won't {\em be} a feasible solution. Then again, the constraint also means that
we can use the expression data to estimate the sort matrix (as many other unmixing
papers have noted.)

Arguably, therefore, I should have capitalized the $x$ in the previous formula.
However, I'll now rewrite it as

\[
AX = B, A \ge 0, X \ge 0
\]

Here, $A$, $X$, and $B$ are all matrices, and we want to estimate $A$ and $X$.

We will model the distribution of each entry of $A$ and $X$ with independent gamma
distributions -- the original case, in which the sort matrix is known exactly,
corresponds to $A$ being infinitely pointy gamma distributions. For now, we assume
$B$ is known exactly.

\subsection{ }

When phrased this way, each gene corresponds to a column of $B$, which we can consider
independently. Writing out one row of this for one gene, we have

\[
\sum_j A_{ij} X_{jk} = B_{ik}
\]

If we know $E[A]$, then we can estimate $E[X]$ using the previously-described method.

Presumably, when the estimate of $A$ has noise, then the variance is nonzero, and
the variance of $X$ will go up, both because of the ``shape'' of the region of possible
solutions, and the fact that $A$ is uncertain. We address this by looking at second
moments, squaring both sides:

\[
( \sum_j A_{ij} X_{jk} )^2 = (B_{ik})^2
\]

or

\[
X^TA^TAX = b^Tb
\]

or

\[
AXX^TA^T = bb^T
\]

Since we assumed $A$ and $X$ are independent, this becomes (um, I'm not sure this
is legit)

\[
\sum_j (A_{ij} X_{jk} )^2 = (B_{ik})^2
\]

Again, since they're independent, we can apply Theorem FIXME from the Ross book

\[
\sum_j (A_{ij})^2 (X_{jk} )^2 = (B_{ik})^2
\]

In other words, in terms of second moments, it's a similar linear
problem as before,
so perhaps we can use the same method to estimate $E[(X_{jk})^2]$.







\end{document}


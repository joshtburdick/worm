\documentclass{article}

\usepackage{hyperref}

\newcommand{\funname}[1]{\texttt{#1}}

\title{More on unmixing}
\author{Josh Burdick}

\begin{document}

\maketitle

<<intro,echo=FALSE>>=
library("MASS")
wd = getwd()
setwd("../../../..")
source("git/unmix/ept/practice/simplex_corner.r")
source("git/unmix/ept/gamma.r")
setwd(wd)
setwd("../../../..")
setwd("git/unmix/unmix_comp/src/")
# print(getwd())
source("sampling/cdaCpp.r")
setwd(wd)

@

\section{Another way of estimating marginals}

We have been treating unmixing as a constrained linear system

\[
Ax = b, x \ge 0
\]

This defines a linear space of solutions; we then chose $x$ uniformly
at random from that space, and used that distribution as the range
of possible levels of expression. Previously, we used sampling
and EP to estimate these. EP was much faster, but had convergence
issues. I tried to alleviate the convergence problems by approximating the
marginals using gamma distributions, rather than a multivariate
normal distribution. This is partly motivated by the fact that the
actual marginals (from sampling) look more like gamma distributions
than normal distributions.

Another approach uses the fact that
the solutions to the above form a convex linear space. As such,
if we can find the corners of that region, the boundaries between
the corners are straight lines, and we can linearly transform the
region to the standard unit simplex

\[
\sum y = 1, y \ge 0
\]

Note that this has the same number of ``corners'' as the original
space, which will be less than $n$ (the ``number of cells'', or
length of $x$.) The moments of this are easy to find: if $m$ is
the number of corners,
then each marginal distribution
has a $\mathrm{Beta}(m-1, 1)$ distribution. We then compute the
moments of the original distribution using the inverse of the
linear transformation.

This leaves the problem of finding the corners; for now,
 we just maximize each 
coordinate separately. This gives $n$ points, but only $m$ of them
are unique. We just pick distinct points based on distance.

Each maximization is a linear programming problem, so this isn't
exactly cheap. (In our case, this is 1,341 linear programming problems
per gene.) However, it still seems faster than sampling.

\subsection{Accuracy of that approximation}

To test this approach, we compare its marginals to those from
sampling (at least on a toy problem.)

First, we design a {\em very} toy problem:

<<toy1,echo=TRUE>>=
A = matrix(sample(0:5, 32, replace=TRUE), nrow=4)
A
x = sample(c(0,1,2,4,8), 8, replace=TRUE)
x
b = as.vector(A %*% x)
b
@

We compute marginals using this method.

<<infer1,echo=TRUE>>=
x.infer = approx.region.simplex.transform(A,b)
round(x.infer,2)
round(as.vector(A %*% x.infer["m",]), 2)
@

So, at least the mean matches the constraints.

<<sample1,echo=FALSE>>=
x0 = x.infer["m",]
x.sample = sample.cda(A, b, x0, 1e5, 100)
@

We then compare with sampling.
By eye, the marginals estimated this way (in blue) fit the actual
densities from sampling (in grey) reasonably. In theory, I think
the mean and variance estimates should be exact; but the gamma
distribution with that mean and variance won't necessarily match
the actual distribution (which won't have exactly a gamma distribution.)

<<argCompare1,dev='pdf',fig.width=8,fig.height=4,echo=FALSE>>=
par(mfrow=c(2,4))
xlim=range(x.sample)
x = xlim[2] * (c(0:100)/100)
g = gamma.mv2s(x.infer)
for(j in 1:8) {
  hist(x.sample[,j], xlim=xlim, breaks=200, col="#00000010", border="#00000020")
  par(new=TRUE)
  plot(x, dgamma(x, shape=g[1,j], rate=g[2,j]), type="l",
    col="#0000ff", lwd=2,
    xlab="", ylab="", main="", xlim=xlim, xaxt="n", yaxt="n")
}
@

\section{Summary}

Finding the mean and variance from the 
original problem, in a deterministic (and probably faster) way is nice.
However, the $x \ge 0$ constraint is problematic: while it gives a way of estimating
uncertainty, it also means that if the sort matrix disagrees too much with the expression
data, there won't {\em be} a feasible solution.

Then again, the constraint also means that
we can use the expression data to estimate the sort matrix. Many other
papers have done this, but not (as far as I know) when the linear
system is underdetermined. I would guess that improving the sort
matrix estimate, at the same time as estimating expression, would
give somewhat higher accuracy. However, I'm not sure how to do that.

\end{document}


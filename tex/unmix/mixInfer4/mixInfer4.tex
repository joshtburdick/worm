\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsmath}
\usepackage{hyperref}

\newcommand{\funname}[1]{\texttt{#1}}

\title{Noise in the sort matrix}
\author{Josh Burdick}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle




\section{Some test data}

Before doing anything else, I define a tiny test data set, for these reasons:

\begin{itemize}

\item Any parameter tuning won't invalidate results on the real data. (Previously,
I was subsetting to, e.g., the most highly expressed genes.)

\item It may be easier to see if unmixing is working on a toy example.

\item Speed is not the biggest concern.

\end{itemize}

Here, the sort matrix is A, and the expression is X.
Note that since rows of A and X all add up to one, this is also
the case for the rows of B (they are all ``row-stochastic.'')


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{apply}\hlstd{(sd0}\hlopt{$}\hlstd{a,} \hlnum{1}\hlstd{, sum)}
\end{alltt}
\begin{verbatim}
## [1] 1 1
\end{verbatim}
\begin{alltt}
\hlkwd{apply}\hlstd{(sd0}\hlopt{$}\hlstd{x,} \hlnum{1}\hlstd{, sum)}
\end{alltt}
\begin{verbatim}
## [1] 1 1 1 1 1
\end{verbatim}
\begin{alltt}
\hlkwd{apply}\hlstd{(sd0}\hlopt{$}\hlstd{b,} \hlnum{1}\hlstd{, sum)}
\end{alltt}
\begin{verbatim}
## [1] 1 1
\end{verbatim}
\end{kframe}
\end{knitrout}

One definition of ``working'' would be: if we unmix using a perturbed prior for
A (but using the {\em actual} sort matrix in simulating B), then the posterior
estimate for A is closer to the actual A than the prior.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{p} \hlkwb{=} \hlkwd{prior.1}\hlstd{(sd0,} \hlnum{10}\hlstd{,} \hlnum{2}\hlstd{)}
\hlcom{# r = unmix.expr.and.sort.matrix.1(p$a.prior, p$x.prior, sd0$b, max.iters=5)}
\end{alltt}
\end{kframe}
\end{knitrout}




\subsection{Distribution of AX}

If we sample from A and X, we should be able to estimate the marginal distributions
of AX by moment-matching...





\section{EP, with an approximate sort matrix}

FIXME this should probably be in a different document.

So far, we've been treating the sort matrix $A$ as exactly known,
which seems like a weak assumption. Therefore, we assume that
its entries are drawn from a gamma distribution, and that
we have a constraint as before.

\begin{align*}
A_{i,j} &\sim \mathrm{Gamma}(\alpha_{i,j},\beta_{i,j}) \\
AX &= b
\end{align*}

If $A$ is known exactly, then I think I know how to approximate
the mean and variance of $X$. However, I don't know how to
modify that method if $A$ is approximate. Therefore, I'm
trying to use EP again.

\subsection{Breaking apart the constraint}

As a toy example of this, we can sample from the
marginals with two constraints, compared to applying
one constraint at a time. This is sort of like assuming
the covariance is diagonal.

If the system is fairly underdetermined, this seems like a
reasonable approximation. Presumably, it will do worse
in a more determined situation (that is, if the number
of sort fractions approaches the number of cells.) We
ignore this for now.

\subsection{Approximating one constraint}

Having done this, we estimate the marginals for each
constraint separately. We do this essentially by
scaling the unit simplex (as I described earlier.)
Without loss of generality, we can assume the constraint
adds up to 1:

\[
A_1 X_1 + A_2 X_2 + ... + A_n X_n = 1
\]

One wrinkle is that $A$ is not known exactly; we assume that
$A$ has a gamma distribution.
This means that $\frac{1}{A_i}$ has
an inverse gamma distribution. If $A$'s shape is at least
2, then we can find the mean and variance of $\frac{1}{A_i}$.
We then moment-match, using the fact that $E[XY] = E[X]E[Y]$.
(The inverse-gamma mean and/or variance aren't well-defined if the
shape is 2 or less.)

(Note that we can't simply {\em divide} the moments of $A$. However,
if we can find the moments of $1/A$, at least approximately, then
we can multiply by them.)

This raises the question of how well a gamma distribution
approximates an inverse gamma distribution. As a quick empirical check
of this, here are some inverse gamma distributions, together with a
moment-matched gamma approximation. (The rate of the inverse gamma presumably
doesn't matter.)


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ig.params} \hlkwb{=} \hlkwd{rbind}\hlstd{(}\hlkwc{a}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{2.1}\hlstd{,}\hlnum{2.5}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{5}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{100}\hlstd{),} \hlkwc{b}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{))}
\hlstd{ig.params}
\end{alltt}
\begin{verbatim}
##   [,1] [,2] [,3] [,4] [,5] [,6]
## a  2.1  2.5    3    5   10  100
## b  1.0  1.0    1    1    1    1
\end{verbatim}
\begin{alltt}
\hlkwd{ig.s2mv}\hlstd{(ig.params)}
\end{alltt}
\begin{verbatim}
##        [,1]      [,2] [,3]       [,4]       [,5]         [,6]
## m 0.9090909 0.6666667 0.50 0.25000000 0.11111111 1.010101e-02
## v 8.2644628 0.8888889 0.25 0.02083333 0.00154321 1.041127e-06
\end{verbatim}
\begin{alltt}
\hlkwd{gamma.mv2s}\hlstd{(}\hlkwd{ig.s2mv}\hlstd{(ig.params))}
\end{alltt}
\begin{verbatim}
##   [,1] [,2] [,3] [,4] [,5] [,6]
## a 0.10 0.50    1    3    8   98
## b 0.11 0.75    2   12   72 9702
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Applying EP}

We now (maybe) have a situation similar to that
described by Minka: several approximations which we can
iteratively improve.




\subsection{Comparison with sampling}


I don't have a strategy for sampling when there's noise
in the sort matrix.


\section{Damping and convergence}

One issue with EP is getting it to converge.

Let $x$ be the (natural) parameters of EP, and $f$ be a function that
does one round of EP updates. Basic EP repeatedly applies $f$ to update $x$.

\[x_{new} \leftarrow f(x)\]

This EP update rule sometimes converges, and sometimes doesn't.
It is only provably known to converge on certain types of problems [CITE Minka].
(In contrast, expectation maximization is guaranteed to converge
to a local maximum of the likelihood, although not neccessarily a global maximum.)

Various modifications have been introduced to address this.
One of the simplest modifications, {\em damping}, reduces how much EP ``moves''
in a particular direction. Let $\alpha$ be the amount of damping, which is
at least 0 (no
damping) and less than 1 (no update.) A damped EP update is

\[x_{new} \leftarrow (1-\alpha)f(x) + \alpha x\]

(FIXME is this the usual $\alpha$ definition?)

Using a sufficiently small $\alpha$ can guarantee convergence, at the expense
of requiring more updates [CITE Minka?]. Often, $\alpha$ is chosen empirically,
by choosing the smallest amount of damping which still converges.

One slightly more rigorous approach to choosing
$\alpha$ is to define an objective function as follows:

\[g(x) = \sum_i |[f(x) - x]_i|\]

$g(x) = 0$ if and only if $x$ is a fixed point of $f$ (which is what we're looking for).
We can apply existing methods to minimize $g$. One option might be to do line search
between $x$ and $f(x)$, minimizing $g$ along this line.


\end{document}


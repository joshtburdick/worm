\documentclass{article}

\usepackage{amsmath}
\usepackage{hyperref}

\newcommand{\funname}[1]{\texttt{#1}}

\title{Noise in the sort matrix}
\author{Josh Burdick}

\begin{document}

\maketitle

<<intro,echo=FALSE,cache=FALSE>>=
opts_chunk$set(cache=TRUE, autodep=TRUE)
library("MASS")
wd = getwd()
setwd("../../../..")
source("git/utils.r")
source("git/unmix/ml/unmix_sort_fraction_ml_1.r")
source("git/unmix/ml/sampleData1.r")

setwd("git/unmix/unmix_comp/src/")
# print(getwd())
# source("sampling/cdaCpp.r")
setwd(wd)

# allow caching
opts_chunk$set(cache=TRUE, autodep=TRUE)
@


\section{Some test data}

Before doing anything else, I define a tiny test data set, for these reasons:

\begin{itemize}

\item Any parameter tuning won't invalidate results on the real data. (Previously,
I was subsetting to, e.g., the most highly expressed genes.)

\item It may be easier to see if unmixing is working on a toy example.

\item Speed is not the biggest concern.

\end{itemize}

Here, the sort matrix is A, and the expression is X.
Note that since rows of A and X all add up to one, this is also
the case for the rows of B (they are all ``row-stochastic.'')


<<sum1,echo=TRUE>>=
apply(sd0$a, 1, sum)
apply(sd0$x, 1, sum)
apply(sd0$b, 1, sum)

@

One definition of ``working'' would be: if we unmix using a perturbed prior for
A (but using the {\em actual} sort matrix in simulating B), then the posterior
estimate for A is closer to the actual A than the prior.

\subsection{Distribution of AX}

If we sample from A and X, we should be able to estimate the marginal distributions
of AX by moment-matching...





\section{EP, with an approximate sort matrix}

FIXME this should probably be in a different document.

So far, we've been treating the sort matrix $A$ as exactly known,
which seems like a weak assumption. Therefore, we assume that
its entries are drawn from a gamma distribution, and that
we have a constraint as before.

\begin{align*}
A_{i,j} &\sim \mathrm{Gamma}(\alpha_{i,j},\beta_{i,j}) \\
AX &= b
\end{align*}

If $A$ is known exactly, then I think I know how to approximate
the mean and variance of $X$. However, I don't know how to
modify that method if $A$ is approximate. Therefore, I'm
trying to use EP again.

\subsection{Breaking apart the constraint}

As a toy example of this, we can sample from the
marginals with two constraints, compared to applying
one constraint at a time. This is sort of like assuming
the covariance is diagonal.

If the system is fairly underdetermined, this seems like a
reasonable approximation. Presumably, it will do worse
in a more determined situation (that is, if the number
of sort fractions approaches the number of cells.) We
ignore this for now.

\subsection{Approximating one constraint}

Having done this, we estimate the marginals for each
constraint separately. We do this essentially by
scaling the unit simplex (as I described earlier.)
Without loss of generality, we can assume the constraint
adds up to 1:

\[
A_1 X_1 + A_2 X_2 + ... + A_n X_n = 1
\]

One wrinkle is that $A$ is not known exactly.

\begin{align*}
A_i X_i &\sim \mathrm{Beta}(1, n-1) \\
X_i &\sim {\mathrm{Beta}(1, n-1)}(\frac{1}{A_i})
\end{align*}


Since $A_i$ has a gamma distribution, $\frac{1}{A_i}$ has
an inverse gamma distribution. If $A$'s shape is at least
1, then we can find the mean and variance of $\frac{1}{A_i}$.
We then moment-match, using the fact that $E[XY] = E[X]E[Y]$.


\subsection{Applying EP}

We now (maybe) have a situation similar to that
described by Minka: several approximations which we can
iteratively improve.





\subsection{Comparison with sampling}


I don't have a strategy for sampling when there's noise
in the sort matrix.


\end{document}

